{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DATA_DIR = os.getcwd()\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B\" \n",
    "SAVE_PATH = 'Llama-3.2-3B'  # Output file name\n",
    "HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\n",
    "chunk_size = 1000\n",
    "# assumes that saved model embedding table before\n",
    "SAVE_PATH_EMB = os.path.join(DATA_DIR, \"embed_table.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookup_table(save_path):\n",
    "    embed_table = torch.load(SAVE_PATH_EMB).to('cuda')\n",
    "    num_vocab = embed_table.shape[0]\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Generating table: num: {num_vocab}, type: {embed_table.dtype}\")\n",
    "\n",
    "    # Create a memory-mapped NumPy array\n",
    "    sim_table = np.memmap(save_path + \".mmap\", dtype=np.float16, mode='w+', shape=(num_vocab, num_vocab))\n",
    "    file_path = save_path + \".mmap\"\n",
    "    actual_size_bytes_after_memmap = os.path.getsize(file_path)\n",
    "    actual_size_gb_after_memmap = actual_size_bytes_after_memmap / (1024 * 1024 * 1024)\n",
    "    print(f\"Actual file size (after memmap creation): {actual_size_gb_after_memmap:.2f} GB\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, num_vocab, chunk_size), desc=\"Computing Lookup Table (Chunks)\"):\n",
    "            chunk_i_end = min(i + chunk_size, num_vocab)\n",
    "            chunk_i_embed = embed_table[i:chunk_i_end].to('cuda')\n",
    "            for j in range(0, num_vocab, chunk_size):\n",
    "                chunk_j_end = min(j + chunk_size, num_vocab)\n",
    "                chunk_j_embed = embed_table[j:chunk_j_end].to('cuda')\n",
    "                # Alternative cosine similarity calculation\n",
    "                sim_scores = torch.matmul(chunk_i_embed, chunk_j_embed.T).to('cpu').numpy().astype(np.float16)\n",
    "\n",
    "                sim_table[i:chunk_i_end, j:chunk_j_end] = sim_scores\n",
    "                sim_table.flush()\n",
    "\n",
    "            del chunk_i_embed\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Lookup table saved to {save_path}.mmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lookup_table(SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infini-gram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
